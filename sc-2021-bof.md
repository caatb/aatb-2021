# Supercomputing 2021 Architecture Testbed BoF

This Birds of Feather session will be held on Wednesday, November 17th, 2021 at 12:15 CST. Please use the link below to join in the Zoom session via HUBB. Note that this BoF is **virtual only** - there will be no participants on site in St. Louis!

[SC21 BoF webpage](https://sc21.supercomputing.org/presentation/?id=bof140&sess=sess385)

## Panelists: 
* Alice Koniges - [Maui High Performance Computing Center](https://www.mhpcc.hpc.mil/) 
* Kevin Barker - Pacific Northwest National Laboratory - [CENATE](https://www.pnnl.gov/projects/cenate)
* Mark Klein - [CSCS](https://www.cscs.ch/) 
* Jeff Vetter - ORNL - [ExCL](https://excl.ornl.gov/)
* Jim Laros - Sandia National Laboratories - [HAAPS](https://www.sandia.gov/asc/advanced-simulation-and-computing/computational-systems/haaps/)
* Jeffrey Young - Georgia Tech - [Rogues Gallery](https://crnch-rg.cc.gatech.edu/)

## Seed Questions for Discussion
1) What has the COVID pandemic taught us about supporting remote users for our testbeds? Do we still require "hands on" testing for some equipment, or can we manage most novel architectures remotely?
2) Have we learned anything interesting or unique about how users are interacting with these testbeds now that has changed in the past 2-3 years? Any new trends in HW or tool requests?
3) Has the arrival of exascale computing changed your testbed's focus in any way? Where do you see important developments over the next 3-5 years?



### Questions around scale:
* What is the optimal size for a testbed given financial constraints and why?
* What tools/methods are used to gain insight into performance at larger scales than are directly measurable?

### Questions around infrastructure:
* Should support systems (e.g., I/O) be considered in testbed installations?
* What are some strategies to assess testbed capabilities given an immature software stack?

### Questions around measurement and assessment:
* What are some ways to gain insight into the “lower-level” details of performance and power consumption (or other metrics)?  I.e., are “probes” necessary to gain insight into sub-system behavior? How can these “probes” be deployed? 
* Are common benchmark suites possible given various levels of system maturity?  How should these be formulated?

### Questions around subsystem architecture (e.g., processing, memory, networking, storage):
* Can focused benchmarks targeting individual subsystems be useful in gaining overall system performance knowledge?
* What are some strategies to build a “virtual” node or system level architecture given multiple testbeds covering different architectural components?
* How can we enable better decisions about which accelerator to use (NVIDIA, AMD, Intel, Arm) without running large apps across all of them? Are mini-apps sufficient?
